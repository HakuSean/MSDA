# AlexNet
# -------------------------- Data loader
layer {
  name: "data-source1"
  type: "ImageData"
  top: "data-source1"
  top: "labels-source1"
  include {
    phase: TRAIN
  }
  transform_param {
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123

  }
  image_data_param {
    batch_size: 128
    source: "/home/alfa/Documents/msda/mywork/data/office/office_a.txt"
    shuffle: true
    is_color: true
    new_height: 256
    new_width: 256
  
  }
}
layer {
  name: "data-source2"
  type: "ImageData"
  top: "data-source2"
  top: "labels-source2"
  include {
    phase: TRAIN
  }
  transform_param {
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123

  }
  image_data_param {
    batch_size: 128
    source: "/home/alfa/Documents/msda/mywork/data/office/office_d.txt"
    shuffle: true
    is_color: true
    new_height: 256
    new_width: 256
  
  }
}

layer {
  name: "data-target"
  type: "ImageData"
  top: "data-target"
  top: "labels-target"  # the labels are silenced. The authors did not want to change data layer
  include {
    phase: TRAIN
  }
  transform_param {
    crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123

  }
  image_data_param {
    batch_size: 128
    source: "/home/alfa/Documents/msda/mywork/data/office/office_w.txt"
    shuffle: true
    is_color: true
    new_height: 256
    new_width: 256
  
  }
}


layer {
  name: "data"
  type: "ImageData"
  top: "data"
  top: "labels"
  include {
    phase: TEST
  }
  transform_param {
   crop_size: 227
    mean_value: 104
    mean_value: 117
    mean_value: 123

  }
  image_data_param {
    batch_size: 1
    source: "/home/alfa/Documents/msda/mywork/data/office/office_w.txt"
    shuffle: true
    is_color: true
    new_height: 256
    new_width: 256
  
  }
}

# -------------------- Data concat

layer {
  name: "data-concat2"
  type: "Concat"
  bottom: "data-source1"
  bottom: "data-source2"
  bottom: "data-target"
  top: "data"
  include {
    phase: TRAIN
  }
  concat_param {
    axis: 0
  }
}

layer {
  name: "label-concat"
  type: "Concat"
  bottom: "labels-source1"
  bottom: "labels-source2"
  top: "labels"
 include {
    phase: TRAIN
  }
  concat_param {
    axis: 0
  }
}

# ------------------- start conv (conv layers are kept unchanged)

layer{
	name: "conv1"
	type: "Convolution"
	bottom: "data"
	top: "conv1"
	param {
		lr_mult: 0.0
		decay_mult: 0.0
 	}
	param {
		lr_mult: 0.0
		decay_mult: 0.0
 	}
	convolution_param {
		num_output: 96
		bias_term: true
		pad: 0
		kernel_size: 11
		stride: 4
		weight_filler {
      			type: "msra" # special type of weight filler
    		}
    		bias_filler {
      			type: "constant"
      			value: 0.0
	  		}
  	}
}

layer{
	name: "relu1"
	type: "ReLU"
	bottom: "conv1"
	top: "conv1"
}

layer{
	name: "norm1"
	type: "LRN"
	bottom: "conv1"
	top: "norm1"
	lrn_param {
		local_size: 5
		alpha: 0.0001
		beta: 0.75
  	}
}

layer{
	name: "pool1"
	type: "Pooling"
	bottom: "norm1"
	top: "pool1"
	pooling_param {
		pool: MAX
		kernel_size: 3
		stride: 2
		pad: 0
    	}
}

layer{
	name: "conv2"
	type: "Convolution"
	bottom: "pool1"
	top: "conv2"
	param {
		lr_mult: 0.0
		decay_mult: 0.0
 	}
	param {
		lr_mult: 0.0
		decay_mult: 0.0
 	}
	convolution_param {
		num_output: 256
		bias_term: true
		pad: 2
		kernel_size: 5
		stride: 1
	    	group: 2
		weight_filler {
      			type: "msra"
    		}
    		bias_filler {
      			type: "constant"
      			value: 0.0
	  		}
  	}
}

layer{
	name: "relu2"
	type: "ReLU"
	bottom: "conv2"
	top: "conv2"
}

layer{
	name: "norm2"
	type: "LRN"
	bottom: "conv2"
	top: "norm2"
	lrn_param {
		local_size: 5
		alpha: 0.0001
		beta: 0.75
  	}
}

layer{
	name: "pool2"
	type: "Pooling"
	bottom: "norm2"
	top: "pool2"
	pooling_param {
		pool: MAX
		kernel_size: 3
		stride: 2
		pad: 0
    	}
}

layer{
	name: "conv3"
	type: "Convolution"
	bottom: "pool2"
	top: "conv3"
	param {
		lr_mult: 0.0
		decay_mult: 0.0
 	}
	param {
		lr_mult: 0.0
		decay_mult: 0.0
 	}
	convolution_param {
		num_output: 384
		bias_term: true
		pad: 1
		kernel_size: 3
		stride: 1
		weight_filler {
      			type: "msra"
    		}
    		bias_filler {
      			type: "constant"
      			value: 0.0
	  		}
  	}
}

layer{
	name: "relu3"
	type: "ReLU"
	bottom: "conv3"
	top: "conv3"
}

layer{
	name: "conv4"
	type: "Convolution"
	bottom: "conv3"
	top: "conv4"
	param {
		lr_mult: 0.0
		decay_mult: 0.0
 	}
	param {
		lr_mult: 0.0
		decay_mult: 0.0
 	}
	convolution_param {
		num_output: 384
		bias_term: true
		pad: 1
		kernel_size: 3
		stride: 1
	    	group: 2
		weight_filler {
      			type: "msra"
    		}
    		bias_filler {
      			type: "constant"
      			value: 0.0
	  		}
  	}
}

layer{
	name: "relu4"
	type: "ReLU"
	bottom: "conv4"
	top: "conv4"
}

layer{
	name: "conv5"
	type: "Convolution"
	bottom: "conv4"
	top: "conv5"
	param {
		lr_mult: 0.0
		decay_mult: 0.0
 	}
	param {
		lr_mult: 0.0
		decay_mult: 0.0
 	}
	convolution_param {
		num_output: 256
		bias_term: true
		pad: 1
		kernel_size: 3
		stride: 1
	    	group: 2
		weight_filler {
      			type: "msra"
    		}
    		bias_filler {
      			type: "constant"
      			value: 0.0
	  		}
  	}
}

layer{
	name: "relu5"
	type: "ReLU"
	bottom: "conv5"
	top: "conv5"
}

layer{
	name: "pool5"
	type: "Pooling"
	bottom: "conv5"
	top: "pool5"
	pooling_param {
		pool: MAX
		kernel_size: 3
		stride: 2
		pad: 0
    	}
}


# --------------- this relu is not useful
layer{
	name: "w_relu1"
	type: "ReLU"
	bottom: "pool5"
	top: "pool5_r1"
	include {
		phase: TRAIN
	}
}

layer{
	name: "w_glb"
	type: "Pooling"
	bottom: "pool5_r1"
	top: "pool5_glb"
	include {
		phase: TRAIN
	}
	pooling_param {
		pool: AVE
		global_pooling: true
    	}
}

layer{
	name: "slice_assignment"
	type: "Slice"
	bottom: "pool5_glb"
	top: "pool5_glb_source"
	top: "pool5_glb_target"
	include {
		phase: TRAIN
	}
	slice_param {
		axis: 0
		slice_point: 256 # 256 for source, the rest 128 are target
	}
}

# -------------------------- source predictor (like view predictor)
layer{
	name: "predictor-source"
	type: "InnerProduct"
	bottom: "pool5_glb_source"
	top: "w_score_source"
	param {
	name: "wpredictor-source"
		lr_mult: 10.0
		decay_mult: 1.0
 	}
	param {
	name: "bpredictor-source"
		lr_mult: 20.0
		decay_mult: 0.0
 	}
	include {
		phase: TRAIN
	}
	inner_product_param {
		num_output: 2 # the number of input sources, which is the k
		weight_filler {
      			type: "gaussian"
      			std: 0.01
    		}
    		bias_filler {
      			type: "constant"
      			value: 0.0
    		}
  	}
}

layer {
  	name: "silence_pool5_glb_target"  # do not need target in the domain classifier
  	type: "Silence"
  	bottom: "pool5_glb_target"
	include {
		phase: TRAIN
	}
}

layer{
	name: "entropy_w-source" # here they only use the entropy, do not use domain label
	type: "EntropyLoss"
	bottom: "w_score_source"
	top: "entropy_w-source"
	loss_weight: 0.3 # 0.3
	include {
		phase: TRAIN
	}
}

layer{
	name: "w_soft-source"
	type: "Softmax"
	bottom: "w_score_source"
	top: "w-source"
	include {
		phase: TRAIN
	}
	softmax_param {
		axis: 1 # the softmax is conducted on axis 1 (channel, by default), so it will generate the scores based on each channel.
  	}
}

layer{
	name: "w_slicers"
	type: "Slice"
	bottom: "w-source"
	top: "ws1"
	top: "ws2"
	include {
		phase: TRAIN
	}
	slice_param {
		axis: 1 # here it is in axis 1, which is the channel, indicating the scores (probabilities) in domain 1 or 2
	}
}

# ---------------------------- fc layer (same as previous link from pool 5)
layer{
	name: "fc6"
	type: "InnerProduct"
	bottom: "pool5"
	top: "fc6"
	param {
	name: "wfc6"
		lr_mult: 1.0
		decay_mult: 1.0
 	}
	param {
	name: "bfc6"
		lr_mult: 2.0
		decay_mult: 0.0
 	}
	inner_product_param {
		num_output: 4096
		weight_filler {
      			type: "msra"
    		}
    		bias_filler {
      			type: "constant"
      			value: 0.0
    		}
  	}
}

layer{
	name: "slicer_fc6"
	type: "Slice"
	bottom: "fc6"
	top: "fc6_source"
	top: "fc6_target"
	include {
		phase: TRAIN
	}
	slice_param {
		axis: 0
		slice_point: 256
	}
}

layer{
	name: "wbn_6s1"
	type: "MultiModalBatchNorm"
	bottom: "fc6_source"
	bottom: "ws1"
	top: "fc6_wbns1"
	include {
		phase: TRAIN
	}
	multimodal_batch_norm_param {
		moving_average_fraction: 0.95
	  }
	}

layer{
	name: "wbn_6s2"
	type: "MultiModalBatchNorm"
	bottom: "fc6_source"
	bottom: "ws2"
	top: "fc6_wbns2"
	include {
		phase: TRAIN
	}
	multimodal_batch_norm_param {
		moving_average_fraction: 0.95
	  }
	}

layer{
	name: "wbn_6_sum_source"
	type: "Eltwise"
	bottom: "fc6_wbns1"
	bottom: "fc6_wbns2"
	top: "fc6_wbns"
	include {
		phase: TRAIN
	}
	eltwise_param {
		operation:SUM
		coeff: 1
		coeff: 1
  	}
}

layer{
	name: "bnt_wbn_6"
	type: "BatchNorm"
	bottom: "fc6_target"
	top: "fc6_wbnt"
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	include {
		phase: TRAIN
	}
	batch_norm_param {
		moving_average_fraction: 0.95
		
  	}
}

layer{
	name: "bnt_wbn_6"
	type: "BatchNorm"
	bottom: "fc6"
	top: "fc6_wbn"
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	include {
		phase: TEST
	}
	batch_norm_param {
		moving_average_fraction: 0.95
		
  	}
}

layer{
	name: "concat_wbn_6"
	type: "Concat"
	bottom: "fc6_wbns"
	bottom: "fc6_wbnt"
	top: "fc6_wbn"
	include {
		phase: TRAIN
	}
	concat_param {
		axis: 0
  	}
}

layer{
	name: "sb_6"
	type: "Scale"
	bottom: "fc6_wbn"
	top: "fc6_sb"
	param {
		lr_mult: 1.0
		decay_mult: 0.0
 	}
	param {
		lr_mult: 1.0
		decay_mult: 0.0
 	}
	scale_param {
		filler {
      			type: "constant"
      			value: 1
   		}
    		bias_filler {
      			type: "constant"
      			value: 0
    		}
		axis: 1
    		num_axes: 1
   		bias_term: true
  	}
}

layer{
	name: "relu6"
	type: "ReLU"
	bottom: "fc6_sb"
	top: "fc6_r"
}

# ------------------------------ layer fc7, the operations are similar in layer fc6

layer{
	name: "fc7"
	type: "InnerProduct"
	bottom: "fc6_r"
	top: "fc7"
	param {
	name: "wfc7"
		lr_mult: 1.0
		decay_mult: 1.0
 	}
	param {
	name: "bfc7"
		lr_mult: 2.0
		decay_mult: 0.0
 	}
	inner_product_param {
		num_output: 4096
		weight_filler {
      			type: "msra"
    		}
    		bias_filler {
      			type: "constant"
      			value: 0.0
    		}
  	}
}

layer{
	name: "slicer_fc7"
	type: "Slice"
	bottom: "fc7"
	top: "fc7_source"
	top: "fc7_target"
	include {
		phase: TRAIN
	}
	slice_param {
		axis: 0
		slice_point: 256
	}
}

layer{
	name: "wbn_7s1"
	type: "MultiModalBatchNorm"
	bottom: "fc7_source"
	bottom: "ws1"
	top: "fc7_wbns1"
	include {
		phase: TRAIN
	}
	multimodal_batch_norm_param {
		moving_average_fraction: 0.95
	  }
	}

layer{
	name: "wbn_7s2"
	type: "MultiModalBatchNorm"
	bottom: "fc7_source"
	bottom: "ws2"
	top: "fc7_wbns2"
	include {
		phase: TRAIN
	}
	multimodal_batch_norm_param {
		moving_average_fraction: 0.95
	  }
	}

layer{
	name: "wbn_7_sum_source"
	type: "Eltwise"
	bottom: "fc7_wbns1"
	bottom: "fc7_wbns2"
	top: "fc7_wbns"
	include {
		phase: TRAIN
	}
	eltwise_param {
		operation:SUM
		coeff: 1
		coeff: 1
  	}
}

layer{
	name: "bnt_wbn_7"
	type: "BatchNorm"
	bottom: "fc7_target"
	top: "fc7_wbnt"
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	include {
		phase: TRAIN
	}
	batch_norm_param {
		moving_average_fraction: 0.95
		
  	}
}

layer{
	name: "bnt_wbn_7"
	type: "BatchNorm"
	bottom: "fc7"
	top: "fc7_wbn"
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	include {
		phase: TEST
	}
	batch_norm_param {
		moving_average_fraction: 0.95
		
  	}
}

layer{
	name: "concat_wbn_7"
	type: "Concat"
	bottom: "fc7_wbns"
	bottom: "fc7_wbnt"
	top: "fc7_wbn"
	include {
		phase: TRAIN
	}
	concat_param {
		axis: 0
  	}
}

layer{
	name: "sb_7"
	type: "Scale"
	bottom: "fc7_wbn"
	top: "fc7_sb"
	param {
		lr_mult: 1.0
		decay_mult: 0.0
 	}
	param {
		lr_mult: 1.0
		decay_mult: 0.0
 	}
	scale_param {
		filler {
      			type: "constant"
      			value: 1
   		}
    		bias_filler {
      			type: "constant"
      			value: 0
    		}
		axis: 1
    		num_axes: 1
   		bias_term: true
  	}
}

layer{
	name: "relu7"
	type: "ReLU"
	bottom: "fc7_sb"
	top: "fc7_r"
}

# --------------------------- classifier, also need to do the same operation before loss

layer{
	name: "classifier"
	type: "InnerProduct"
	bottom: "fc7_r"
	top: "scorer"
	param {
	name: "wclassifier"
		lr_mult: 10.0
		decay_mult: 1.0
 	}
	inner_product_param {
		num_output: 31
		bias_term: false
		weight_filler {
      			type: "gaussian"
      			std: 0.01
    		}
  	}
}

layer{
	name: "slicer_scorer"
	type: "Slice"
	bottom: "scorer"
	top: "scorer_source"
	top: "scorer_target"
	include {
		phase: TRAIN
	}
	slice_param {
		axis: 0
		slice_point: 256
	}
}

layer{
	name: "wbn_scs1"
	type: "MultiModalBatchNorm"
	bottom: "scorer_source"
	bottom: "ws1"
	top: "score_wbns1"
	include {
		phase: TRAIN
	}
	multimodal_batch_norm_param {
		moving_average_fraction: 0.95
	  }
	}

layer{
	name: "wbn_scs2"
	type: "MultiModalBatchNorm"
	bottom: "scorer_source"
	bottom: "ws2"
	top: "score_wbns2"
	include {
		phase: TRAIN
	}
	multimodal_batch_norm_param {
		moving_average_fraction: 0.95
	  }
	}

layer{
	name: "wbn_sc_sum_source"
	type: "Eltwise"
	bottom: "score_wbns1"
	bottom: "score_wbns2"
	top: "score_wbns"
	include {
		phase: TRAIN
	}
	eltwise_param {
		operation:SUM
		coeff: 1
		coeff: 1
  	}
}

layer{
	name: "bnt_wbn_sc"
	type: "BatchNorm"
	bottom: "scorer_target"
	top: "score_wbnt"
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	include {
		phase: TRAIN
	}
	batch_norm_param {
		moving_average_fraction: 0.95
		
  	}
}

layer{
	name: "bnt_wbn_sc"
	type: "BatchNorm"
	bottom: "scorer"
	top: "score_wbn"
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	param {
		lr_mult: 0.0
 	}
	include {
		phase: TEST
	}
	batch_norm_param {
		moving_average_fraction: 0.95
		
  	}
}

layer{
	name: "concat_wbn_sc"
	type: "Concat"
	bottom: "score_wbns"
	bottom: "score_wbnt"
	top: "score_wbn"
	include {
		phase: TRAIN
	}
	concat_param {
		axis: 0
  	}
}

layer{
	name: "sb_score"
	type: "Scale"
	bottom: "score_wbn"
	top: "score_sb"
	param {
		lr_mult: 1.0
		decay_mult: 0.0
 	}
	param {
		lr_mult: 1.0
		decay_mult: 0.0
 	}
	scale_param {
		filler {
      			type: "constant"
      			value: 1
   		}
    		bias_filler {
      			type: "constant"
      			value: 0
    		}
		axis: 1
    		num_axes: 1
   		bias_term: true
  	}
}

layer{
	name: "slicer_score"
	type: "Slice"
	bottom: "score_sb"
	top: "score_source"
	top: "score_target"
	include {
		phase: TRAIN
	}
	slice_param {
		axis: 0
		slice_point: 256
	}
}

layer{
	name: "loss"
	type: "SoftmaxWithLoss"
	bottom: "score_source"
	bottom: "labels"
	top: "loss"
	loss_weight: 1  # by default 1
	include {
		phase: TRAIN
	}
}

layer{
	name: "entropy"
	type: "EntropyLoss"
	bottom: "score_target"
	top: "entropy"
	loss_weight: 0.5 # 0.8
	include {
		phase: TRAIN
	}
}

layer{
	name: "accuracy"
	type: "Accuracy"
	bottom: "score_sb"
	bottom: "labels"
	top: "accuracy"
	include {
		phase: TEST
	}
}
layer {
  	name: "silence_target"
  	type: "Silence"
  	bottom: "labels-target"
	include {
		phase: TRAIN
	}
}
